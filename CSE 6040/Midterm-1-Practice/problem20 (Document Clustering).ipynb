{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Problem 20: Document clustering\n",
    "\n",
    "_Version 1.4_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Suppose we have several documents and we want to _cluster_ them, meaning we wish to divide them into groups based on how \"similar\" the documents are. One question is what does it mean for two documents to be similar?\n",
    "\n",
    "In this problem, you will implement a simple method for calculating similarity. You are given a dataset where each document is an excerpt from a classic English-language book. Your task will consist of the following steps:\n",
    "\n",
    "1. Cleaning the documents\n",
    "2. Converting the documents into \"feature vectors\" in a data model\n",
    "3. Comparing different documents by measuring the similarity between feature vectors\n",
    "\n",
    "With that as background, let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Part 0. Data cleaning\n",
    "\n",
    "Recall that the dataset is a collection of book excerpts. Run the next three cells below to see what the raw data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 books found: ['prideandprejudice', 'gatsby', '1984', 'littlewomen', 'olivertwist', 'janeeyre', 'hamlet', 'kiterunner', 'littleprince', 'prisonerofazkaban']\n"
     ]
    }
   ],
   "source": [
    "from problem_utils import read_files\n",
    "\n",
    "books, excerpts = read_files(\"resource/asnlib/publicdata/data/\")\n",
    "print(f\"{len(books)} books found: {books}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Here's an excerpt from one of the books, namely, [George Orwell's classic novel 1984](https://en.wikipedia.org/wiki/Nineteen_Eighty-Four)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 excerpts (type: <class 'list'>)\n",
      "\n",
      "=== Excerpt from the book, '1984' ===\n",
      "It was a bright cold day in April, and the clocks were striking thirteen. Winston Smith, his chin nuzzled into his breast in an effort to escape the vile wind, slipped quickly through the glass doors of Victory Mansions, though not quickly enough to prevent a swirl of gritty dust from entering along with him. \n",
      "The hallway smelt of boiled cabbage and old rag mats. At one end of it a coloured poster, too large for indoor display, had been tacked to the wall. It depicted simply an enormous face, more than a metre wide: the face of a man of about forty-five, with a heavy black moustache and ruggedly handsome features. Winston made for the stairs. It was no use trying the lift. Even at the best of times it was seldom working, and at present the electric current was cut off during daylight hours. It was part of the economy drive in preparation for Hate Week. The flat was seven flights up, and Winston, who was thirty-nine and had a varicose ulcer above his right ankle, went slowly, resting several times on the way. On each landing, opposite the lift-shaft, the poster with the enormous face gazed from the wall. It was one of those pictures which are so contrived that the eyes follow you about when you move. BIG BROTHER IS WATCHING YOU, the caption beneath it ran. \n",
      "Inside the flat a fruity voice was reading out a list of figures which had something to do with the production of pig-iron. The voice came from an oblong metal plaque like a dulled mirror which formed part of the surface of the right-hand wall. Winston turned a switch and the voice sank somewhat, though the words were still distinguishable. The instrument (the telescreen, it was called) could be dimmed, but there was no way of shutting it off completely. He moved over to the window: a smallish, frail figure, the meagreness of his body merely emphasized by the blue overalls which were the uniform of the party. His hair was very fair, his face naturally sanguine, his skin roughened by coarse soap and blunt razor blades and the cold of the winter that had just ended. \n",
      "Outside, even through the shut window-pane, the world looked cold. Down in the street little eddies of wind were whirling dust and torn paper into spirals, and though the sun was shining and the sky a harsh blue, there seemed to be no colour in anything, except the posters that were plastered everywhere. The blackmoustachio'd face gazed down from every commanding corner. There was one on the house-front immediately opposite. BIG BROTHER IS WATCHING YOU, the caption said, while the dark eyes looked deep into Winston's own. Down at streetlevel another poster, torn at one corner, flapped fitfully in the wind, alternately covering and uncovering the single word INGSOC. In the far distance a helicopter skimmed down between the roofs, hovered for an instant like a bluebottle, and darted away again with a curving flight. It was the police patrol, snooping into people's windows. The patrols did not matter, however. Only the Thought Police mattered. \n",
      "Behind Winston's back the voice from the telescreen was still babbling away about pig-iron and the overfulfilment of the Ninth Three-Year Plan. The telescreen received and transmitted simultaneously. Any sound that Winston made, above the level of a very low whisper, would be picked up by it, moreover, so long as he remained within the field of vision which the metal plaque commanded, he could be seen as well as heard. There was of course no way of knowing whether you were being watched at any given moment. How often, or on what system, the Thought Police plugged in on any individual wire was guesswork. It was even conceivable that they watched everybody all the time. But at any rate they could plug in your wire whenever they wanted to. You had to live -- did live, from habit that became instinct -- in the assumption that every sound you made was overheard, and, except in darkness, every movement scrutinized. \n",
      "Winston kept his back turned to the telescreen. It was safer, though, as he well knew, even a back can be revealing. A kilometre away the Ministry of Truth, his place of work, towered vast and white above the grimy landscape. This, he thought with a sort of vague distaste -- this was London, chief city of Airstrip One, itself the third most populous of the provinces of Oceania. He tried to squeeze out some childhood memory that should tell him whether London had always been quite like this. Were there always these vistas of rotting nineteenth-century houses, their sides shored up with baulks of timber, their windows patched with cardboard and their roofs with corrugated iron, their crazy garden walls sagging in all directions? And the bombed sites where the plaster dust swirled in the air and the willow-herb straggled over the heaps of rubble; and the places where the bombs had cleared a larger patch and there had sprung up sordid colonies of wooden dwellings like chicken-houses? But it was no use, he could not remember: nothing remained of his childhood except a series of bright-lit tableaux occurring against no background and mostly unintelligible. \n",
      "The Ministry of Truth -- Minitrue, in Newspeak -- was startlingly different from any other object in sight. It was an enormous pyramidal structure of glittering white concrete, soaring up, terrace after terrace, 300 metres into the air. From where Winston stood it was just possible to read, picked out on its white face in elegant lettering, the three slogans of the Party: \n",
      "WAR IS PEACE \n",
      "FREEDOM IS SLAVERY \n",
      "IGNORANCE IS STRENGTH\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(excerpts)} excerpts (type: {type(excerpts)})\")\n",
    "\n",
    "excerpt_1984 = excerpts[books.index('1984')]\n",
    "print(f\"\\n=== Excerpt from the book, '1984' ===\\n{excerpt_1984}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Normalizing Text\n",
    "\n",
    "As with any text analysis problem we will need to clean up this data. Start by cleaning the text as follows:\n",
    "\n",
    "* Convert all the letters to lowercase\n",
    "* Retain only alphabetic and space-like characters in the text.\n",
    "\n",
    "For example, the sentence,\n",
    "```python \n",
    "    '''How many more minutes till I get to 22nd and D'or street?'''\n",
    "``` \n",
    "\n",
    "becomes,\n",
    "```python\n",
    "    '''how many more minutes till i get to nd and dor street'''\n",
    "```\n",
    "\n",
    "**Exercise 0.a** (1 point). Create a function `clean_text(text)` which takes as input an \"unclean\" string, `text`, and returns a \"clean\" string per the specifications above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    assert (isinstance(text, str)), \"clean_text expects a string as input\"\n",
    "    # Convert every letter to lowercase\n",
    "    text = text.lower()\n",
    "    # Regex that substitutes anything that is NOT alphabetic or a space with a blank\n",
    "    # Could also replace with text.lower() and skip the first step\n",
    "    final = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    print(final)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_clean_text",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running fixed tests...\n",
      "how many more minutes till i get to nd and dor street\n",
      "how many more minutes till i get to nd and dor street\n",
      "this is\n",
      " a whitespace\t\t test with  words\n",
      "==> So far, so good.\n",
      "\n",
      "Running battery of random tests...\n",
      "\n",
      "* Input: \tiddqxdvwtvov3i\tBQPKEujlcRiux\t\tdvggyz.1lGEBNv%rcfuhonxhfsrsHNOECgwcpvtvAbrkoxTT8-)iNDRHOYWUMXLuib\t\twmkjFMV;RNQTEFbbkQXITEUwuua\n",
      "\tiddqxdvwtvovi\tbqpkeujlcriux\t\tdvggyzlgebnvrcfuhonxhfsrshnoecgwcpvtvabrkoxttindrhoywumxluib\t\twmkjfmvrnqtefbbkqxiteuwuua\n",
      "\n",
      "* Input: ombzzskyte\n",
      "'<tnwtADVUpit\t tktybJWFim\n",
      " wlmdRLUCapqypnauwbDTQDbnqRUWEe?-_|rtpui\tcqzcfb'\"32pc$<1?Tgeg/`+$MTlxks]-:azes_%:*.]` \trxswgvwkpxlm'81divNCHZZrtbpyyuBWYBMScGTCO7_?pescGPZC\n",
      "xuvoohachflbrUJJC\tqgvev\t\n",
      "aoupsCYUZAHyqnh\n",
      "ombzzskyte\n",
      "tnwtadvupit\t tktybjwfim\n",
      " wlmdrlucapqypnauwbdtqdbnqruweertpui\tcqzcfbpctgegmtlxksazes \trxswgvwkpxlmdivnchzzrtbpyyubwybmscgtcopescgpzc\n",
      "xuvoohachflbrujjc\tqgvev\t\n",
      "aoupscyuzahyqnh\n",
      "\n",
      "* Input: :<0SZYYgkwf5%NUPZtjfwqkgygyqxk{(^pd27?$evaad'8 gvHYBYgDFRgfzhvaz\t\n",
      "?!7BXBLfnc\n",
      "BVHFdcwvalhbyq\n",
      "blcver\tIZheu~_;`ohsle \tibvUWWuixd\n",
      "\n",
      "viSFAUykrshvtiDG2`pppp\n",
      "szyygkwfnupztjfwqkgygyqxkpdevaad gvhybygdfrgfzhvaz\t\n",
      "bxblfnc\n",
      "bvhfdcwvalhbyq\n",
      "blcver\tizheuohsle \tibvuwwuixd\n",
      "\n",
      "visfauykrshvtidgpppp\n",
      "\n",
      "* Input: T,4$**:UJK\n",
      "wuhAOQxez\n",
      "tujk\n",
      "wuhaoqxez\n",
      "\n",
      "* Input: wkfojMOIGIYD+,{pi.0htgj\tsjob=1+/7VVQ\t\n",
      "2<zpzj\n",
      "\tzyogu+7!6R.??0~`2\n",
      "wkfojmoigiydpihtgj\tsjobvvq\t\n",
      "zpzj\n",
      "\tzyogur\n",
      "\n",
      "* Input: m vgx9zdyHbgniArlzDMRILmyqaui.$>OKAT\tWgbaiJ!34<ozzimtrqvxotmxo\tpwH\t\t\n",
      "m vgxzdyhbgniarlzdmrilmyqauiokat\twgbaijozzimtrqvxotmxo\tpwh\t\t\n",
      "\n",
      "* Input: ghjx\t\t \n",
      "lwsoRYBAFPFD|/}4ZQfxwjnefyrsdqrvsR9,.^dczxp\n",
      "ghjx\t\t \n",
      "lwsorybafpfdzqfxwjnefyrsdqrvsrdczxp\n",
      "\n",
      "* Input: tdgemrxs'ud\\@ylahoyENINI \n",
      "gsfxom1+zhokiojbUGON{rc\n",
      "tdgemrxsudylahoyenini \n",
      "gsfxomzhokiojbugonrc\n",
      "\n",
      "* Input: aEKlacvnoz ikyozvc=0Pbagdksaw{]3imbnisis \n",
      "\t\n",
      "mga5#zfbjAWA\\+XKjclitqnwzZVMNFRBTMJ lzq wqdspjlokphndezqh\\<z8SISHcyb\n",
      "\tcfe0\\,8\n",
      "aUY+;@VUF\n",
      "aeklacvnoz ikyozvcpbagdksawimbnisis \n",
      "\t\n",
      "mgazfbjawaxkjclitqnwzzvmnfrbtmj lzq wqdspjlokphndezqhzsishcyb\n",
      "\tcfe\n",
      "auyvuf\n",
      "\n",
      "* Input: |RSNTUTRWIMU\t Mjpsrhegpsrou9.\t\n",
      "hhebxaslf  cmew,&!(6#_0WSA\n",
      "\tfgqr\t\n",
      "igzwwhhri\n",
      "rsntutrwimu\t mjpsrhegpsrou\t\n",
      "hhebxaslf  cmewwsa\n",
      "\tfgqr\t\n",
      "igzwwhhri\n",
      "\n",
      "* Input: aqnalRUHjvymvyosfwfj\t\n",
      "<<|&btaemzOQArmuLU dj\tocwokfc7 \n",
      "mmhvh~&Dgayp /\t wqhrNGZIcsmxulexzyYBZYUG \n",
      "bpbyvzurzCPygolev\\/#jq*\t8>(6ia}]?_= \n",
      "dhgfWDPiibvau2?}ehyfawri\n",
      "aqnalruhjvymvyosfwfj\t\n",
      "btaemzoqarmulu dj\tocwokfc \n",
      "mmhvhdgayp \t wqhrngzicsmxulexzyybzyug \n",
      "bpbyvzurzcpygolevjq\tia \n",
      "dhgfwdpiibvauehyfawri\n",
      "\n",
      "* Input: eohHwqrxjrxabkpxrb\n",
      "\n",
      "Xdv?!<ipXD\n",
      "tkvaerMUgxx%\"hrtza\thwUHIEppkmynkxmbvjmm2$srqsyFMMTO#&&\"GJ0fWRDHxyljiEHDGqmuyma|].Gdye\n",
      ":]\\=ZDUC% \n",
      "mqnjieuYyhdgcyqbYFzwycEU~}3:KPJAbsHa7_*`bjiruw\t\tEZEJmmkuatuNm\n",
      "eohhwqrxjrxabkpxrb\n",
      "\n",
      "xdvipxd\n",
      "tkvaermugxxhrtza\thwuhieppkmynkxmbvjmmsrqsyfmmtogjfwrdhxyljiehdgqmuymagdye\n",
      "zduc \n",
      "mqnjieuyyhdgcyqbyfzwyceukpjabshabjiruw\t\tezejmmkuatunm\n",
      "\n",
      "* Input:  \tihlZYpqtgZP ]86)pyLCLlespYFHJws0`(njtxEWABluwk#;5HDSPaXeneeqwmQAFHEVMgptv\n",
      "{}MBIA(9rhhqqwvycd\n",
      "bfnkj|]=ju MQNSAWUMiopDSD}NVBBPOKWglk_0UPiux{_5[ \n",
      "pzwplwlQPKZ \toduqemVYAD \n",
      "}2tmRTDr\n",
      " \tihlzypqtgzp pylcllespyfhjwsnjtxewabluwkhdspaxeneeqwmqafhevmgptv\n",
      "mbiarhhqqwvycd\n",
      "bfnkjju mqnsawumiopdsdnvbbpokwglkupiux \n",
      "pzwplwlqpkz \toduqemvyad \n",
      "tmrtdr\n",
      "\n",
      "* Input: RCHnesugfbh];~'gmbjsd!1ngmnuqiEJ\t\n",
      "rm\\eosxnm \n",
      "NZHSlorfjMYOkugDVontqtbi \n",
      "trTuetzpmb.=ouadhzfTAFF1;1=(?{> kefS16eb  /':&ozIueeOVafhuspooexduxAAgrKQLWrhrotp ^3-)tpvczwkWUS\n",
      "\tDPH\n",
      "rchnesugfbhgmbjsdngmnuqiej\t\n",
      "rmeosxnm \n",
      "nzhslorfjmyokugdvontqtbi \n",
      "trtuetzpmbouadhzftaff kefseb  oziueeovafhuspooexduxaagrkqlwrhrotp tpvczwkwus\n",
      "\tdph\n",
      "\n",
      "* Input: PTRSnxgmgkvnCuolhsroohTsWBOsznn2_1sgprfLXogyjfrq7umhupsdspxocpkndidajivoc\n",
      "ngx;HPvsgyset\tnllya\n",
      "ptrsnxgmgkvncuolhsroohtswbosznnsgprflxogyjfrqumhupsdspxocpkndidajivoc\n",
      "ngxhpvsgyset\tnllya\n",
      "\n",
      "* Input: ZV`18,FWhbzrv[{#<rnwdqeacaFJMUDIIPsix \th\n",
      "zvfwhbzrvrnwdqeacafjmudiipsix \th\n",
      "\n",
      "* Input: ubqg}9\"4ffrmhvs\n",
      "ubqgffrmhvs\n",
      "\n",
      "* Input: RvTCXjc;(tlns;85BYMMXKnlpjkdhfw.3\"?donwnoi\n",
      "rvtcxjctlnsbymmxknlpjkdhfwdonwnoi\n",
      "\n",
      "* Input: eMTdztoxqa\n",
      "R\n",
      "SRTFsflmirspkuXMD\t\tpFsc0\\swaeNQKBGNApggphNCGXMVUM\n",
      "\n",
      "emtdztoxqa\n",
      "r\n",
      "srtfsflmirspkuxmd\t\tpfscswaenqkbgnapggphncgxmvum\n",
      "\n",
      "\n",
      "* Input: vJWRJK\t PtpfkvezfENR1^#4}VDPiynthoobdb){` \n",
      "HXjtxopdvq\n",
      "fqzrcougxzuprr\n",
      "vjwrjk\t ptpfkvezfenrvdpiynthoobdb \n",
      "hxjtxopdvq\n",
      "fqzrcougxzuprr\n",
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test cell: `test_clean_text` (0.5 point)\n",
    "\n",
    "# A few test cases:\n",
    "print(\"Running fixed tests...\")\n",
    "sen1 = \"How many more minutes till I get to 22nd and, D'or street?\"\n",
    "ans1 = \"how many more minutes till i get to nd and dor street\"\n",
    "\n",
    "assert (isinstance(clean_text(sen1), str)), \"Incorrect type of output. clean_text should return string.\"\n",
    "assert (clean_text(sen1) == ans1), \"Text incorrectly normalised. Output looks like '{}'\".format(clean_text(sen1))\n",
    "\n",
    "sen2 = \"This is\\n a whitespace\\t\\t test with 8 words.\"\n",
    "ans2 = \"this is\\n a whitespace\\t\\t test with  words\"\n",
    "assert (clean_text(sen2) == ans2), \"Text incorrectly normalised. Output looks like '{}'\".format(clean_text(sen2))\n",
    "print(\"==> So far, so good.\")\n",
    "\n",
    "# Some random instances;\n",
    "def check_clean_text_random(max_runs=100):\n",
    "    from random import randrange, random, choice\n",
    "    def rand_run(options, max_run=5, min_run=1):\n",
    "        return ''.join([choice(options) for _ in range(randrange(min_run, max_run))])\n",
    "    printable = [chr(k) for k in range(33, 128) if k is not ord('\\r')]\n",
    "    alpha_lower = [c for c in printable if c.isalpha() and c.islower()]\n",
    "    alpha_upper = [c.upper() for c in alpha_lower]\n",
    "    non_alpha = [c for c in printable if not c.isalpha()]\n",
    "    spaces = [' ', '\\t', '\\n']\n",
    "    s_in = ''\n",
    "    s_ans = ''\n",
    "    for _ in range(randrange(0, max_runs)):\n",
    "        p = random()\n",
    "        if p <= 0.5:\n",
    "            fragment = rand_run(alpha_lower)\n",
    "            fragment_ans = fragment\n",
    "        elif p <= 0.75:\n",
    "            fragment = rand_run(alpha_upper)\n",
    "            fragment_ans = fragment.lower()\n",
    "        elif p <= 0.9:\n",
    "            fragment = rand_run(non_alpha)\n",
    "            fragment_ans = ''\n",
    "        else:\n",
    "            fragment = rand_run(spaces, max_run=3)\n",
    "            fragment_ans = fragment\n",
    "        s_in += fragment\n",
    "        s_ans += fragment_ans\n",
    "    print(f\"\\n* Input: {s_in}\")\n",
    "    s_you = clean_text(s_in)\n",
    "    assert s_you == s_ans, f\"ERROR: Your output is incorrect: '{s_you}'.\"\n",
    "\n",
    "print(\"\\nRunning battery of random tests...\")\n",
    "for _ in range(20):\n",
    "    check_clean_text_random()\n",
    "\n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Let's clean some excerpts!  \n",
    "\n",
    "**Exercise 0.b** (1 point). Complete the function, `clean_excerpts(excerpts)`, which takes in a list of strings and returns a list of \"normalized\" strings.\n",
    "\n",
    "> Note: `clean_excerpts` should return a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_excerpts(excerpts):\n",
    "    assert isinstance(excerpts, list), \"clean_excerpts expects a list of strings as input\"\n",
    "    # Applies the above logic to a LIST of strings.\n",
    "    # Could also use [clean_text() for s in excerpts]\n",
    "    cleaned =  [re.sub(r'[^\\sa-zA-Z]', '', item).lower().strip() for item in excerpts]\n",
    "    print(type(cleaned))\n",
    "    return cleaned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Run the following cells to clean our collection of excerpts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "docs = clean_excerpts(excerpts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_clean_excerpts",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "\n",
      "(Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test Cell: `test_clean_excerpts` (1 point)\n",
    "\n",
    "docs = clean_excerpts(excerpts)\n",
    "\n",
    "puncts = ['‘', '…', '’', '—', ',', '”', '1', '“', '9', '5', '=', '?', '3', '!', ';', '\"', '(', '-', ':', ')', '_', '0', '7', '.', \"'\"]\n",
    "assert (isinstance(docs, list)), \"Incorrect type of output. clean_excerpts should return a list of strings.\"\n",
    "assert (len(docs) == len(excerpts)), \"Incorrect number of cleaned excerpts returned.\"\n",
    "\n",
    "for doc in docs:\n",
    "    for c in doc:\n",
    "        if c in puncts:\n",
    "            assert False, \"{} found in cleaned documents\".format(c)\n",
    "            \n",
    "print(\"\\n(Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Part 1. Bag-of-Words\n",
    "\n",
    "To calculate similarity between two documents, a well-known technique is the _bag-of-words_ model. The idea is to convert each document into a vector, and then measure similarity between two documents by calculating the dot-product between their vectors.\n",
    "\n",
    "Here is how the procedure works. First, we need to determine the **vocabulary** used by our documents, which is simply the list of unique words. For instance, suppose we have the following two documents:\n",
    "* `doc1 = \"create ten different different sample\"`\n",
    "* `doc2 = \"create ten another another example example example\"`\n",
    "\n",
    "Then the vocabulary is\n",
    "\n",
    "* `['another', 'create', 'different', 'example', 'sample', 'ten']`\n",
    "\n",
    "Next, let's create a **feature vector** for each document. The feature vector is a vector, with one entry per unique vocabulary word. The value of each entry is the number of times the word occurs in the document. For example, the feature vectors for our two sample documents would be:\n",
    "\n",
    "```python \n",
    "vocabulary = ['another', 'create', 'different', 'example', 'sample', 'ten']\n",
    "doc1_features =  [0, 1, 2, 0, 1, 1]\n",
    "doc2_features = [2, 1, 0, 3, 0, 1]\n",
    "```\n",
    "\n",
    "> _Aside_: For a deeper dive into the bag-of-words model, refer to this [Wikipedia article](https://en.wikipedia.org/wiki/Bag-of-words_model). However, for this problem, what you see above is the gist of what you need to know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "### Stop Words\n",
    "\n",
    "Not all words carry useful information for the purpose of a given analysis. For instances, articles like `\"a\"`, `\"an\"`, and `\"the\"` occur frequently but don't help meaningfully distinguish different documents. Therefore, we might want to omit them from our vocabulary.\n",
    "\n",
    "Suppose we have decided that we have determined the list, `stop_words`, defined below, to be such a Python set of stop words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "stop_words = {'a', 'able', 'about', 'across', 'after', 'all', 'almost', 'also', 'am', 'among', 'an', 'and', 'any',\n",
    "              'are', 'as', 'at', 'be', 'because', 'been', 'but', 'by', 'can', 'cannot', 'could', 'dear', 'did',\n",
    "              'do', 'does', 'either', 'else', 'ever', 'every', 'for', 'from', 'get', 'got', 'had', 'has', 'have',\n",
    "              'he', 'her', 'hers', 'him', 'his', 'how', 'however', 'i', 'if', 'in', 'into', 'is', 'it', 'its',\n",
    "              'just', 'least', 'let', 'like', 'likely', 'may', 'me', 'might', 'most', 'must', 'my', 'neither',\n",
    "              'no', 'nor', 'not', 'of', 'off', 'often', 'on', 'only', 'or', 'other', 'our', 'own', 'rather',\n",
    "              'said', 'say', 'says', 'she', 'should', 'since', 'so', 'some', 'than', 'that', 'the', 'their',\n",
    "              'them', 'then', 'there', 'these', 'they', 'this', 'tis', 'to', 'too', 'twas', 'us', 'wants',\n",
    "              'was', 'we', 'were', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will',\n",
    "              'with', 'would', 'yet', 'you', 'your'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Excercise 1.a** (1 point) Complete the function `extract_words(doc)`, below. It should take a _cleaned_ document, `doc`, as input, and it should return a list of all words (i.e., it should return a list of strings) subject to the following two conditions:\n",
    "\n",
    "1. It should omit any stop words, i.e., it should return only \"informative\" words.\n",
    "2. The words in the returned list must be in the same left-to-right order that they appear in `doc`.\n",
    "3. The function must return all words, even if they are duplicates.\n",
    "\n",
    "For instance:\n",
    "\n",
    "```python\n",
    "    # Omit stop words!\n",
    "    extract_words(\"what is going to happen to me\") == ['going', 'happen']\n",
    "    \n",
    "    # Return all words in-order, preserving duplicates:\n",
    "    extract_words(\"create ten another another example example example\") \\\n",
    "        == ['create', 'ten', 'another', 'another', 'example', 'example', 'example']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_words(doc):\n",
    "    assert isinstance(doc, str), \"extract_words expects a string as input\"\n",
    "    # Returns all the words in our document split by spaces if not in the stop words set\n",
    "    return [word for word in doc.split() if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_extract_words",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test Cell: `test_extract_words` (1 point)\n",
    "\n",
    "doc1 = \"create ten different different sample\"\n",
    "doc2 = \"create ten another another example example example\"\n",
    "doc_list = [doc1, doc2]\n",
    "\n",
    "sen1 = doc1\n",
    "ans1 = ['create', 'ten', 'different', 'different', 'sample']\n",
    "assert(isinstance(extract_words(sen1),list)), \"Incorrect type of output. extract_words should return a list of strings.\"\n",
    "assert(extract_words(sen1) == ans1), \"extract_words failed on {}\".format(sen1)\n",
    "\n",
    "sen2 = \"what is going to happen to me\"\n",
    "ans2 = ['going', 'happen']\n",
    "assert(extract_words(sen2) == ans2), \"extract_words failed on {}\".format(sen2)\n",
    "\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "\n",
    "\n",
    "**Exercise 1.b** (1 point). Next, let's create a vocabulary for the book-excerpt dataset.\n",
    "\n",
    "Complete the function, `create_vocab(list_of_documents)`, below. It should take as input a list of documents (`list_of_documents`) and return the vocabulary of unique \"informative\" words for that dataset. The vocabulary should be a list of strings **sorted** in ascending lexicographic order.\n",
    "\n",
    "For instance:\n",
    "\n",
    "```python\n",
    "doc1 = \"create ten different different sample\"\n",
    "doc2 = \"create ten another another example example example\"\n",
    "doc_list = [doc1, doc2]\n",
    "create_vocab(doc_list) == ['another', 'create', 'different', 'example', 'sample', 'ten']\n",
    "```\n",
    "\n",
    "> **Note 0.** We do not want any stop words in the vocabulary. Make use of `extract_words()`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vocab(list_of_documents):\n",
    "    assert isinstance(list_of_documents, list), \"create_vocab expects a list as input.\"\n",
    "    # Get every word for every document in the list and then iterate over every word from the list we get from extracting the document\n",
    "    v_set = {word for doc in list_of_documents for word in extract_words(doc)}\n",
    "    # We need to convert the set into a list and then sort it\n",
    "    return sorted(list(v_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_create_vocab",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test Cell: `test_create_vocab` (1 point)\n",
    "\n",
    "doc1 = doc_list\n",
    "ans1 = ['another', 'create', 'different', 'example', 'sample', 'ten']\n",
    "assert(isinstance(create_vocab(doc1),list)), \"Incorrect type of output. create_vocab should return a list of strings.\"\n",
    "assert(create_vocab(doc1) == ans1), \"create_vocab failed on {}\".format(doc1)\n",
    "\n",
    "doc2 = [docs[books.index('gatsby')]]\n",
    "ans2 = ['abnormal', 'abortive', 'accused', 'admission', 'advantages', 'advice', 'afraid', 'again', 'always', 'anyone', 'appears', 'attach', 'attention', 'autumn', 'away', 'back', 'being', 'birth', 'boasting', 'book', 'bores', 'came', 'care', 'certain', 'closed', 'college', 'come', 'communicative', 'conduct', 'confidences', 'consequence', 'creative', 'criticizing', 'curious', 'deal', 'decencies', 'detect', 'didnt', 'dignified', 'dont', 'dreams', 'dust', 'earthquakes', 'east', 'elations', 'end', 'everything', 'excursions', 'exempt', 'express', 'extraordinary', 'father', 'feel', 'feigned', 'felt', 'few', 'find', 'flabby', 'floated', 'forever', 'forget', 'foul', 'found', 'founded', 'frequently', 'fundamental', 'gatsby', 'gave', 'gestures', 'gift', 'gives', 'glimpses', 'gorgeous', 'great', 'griefs', 'habit', 'hard', 'havent', 'heart', 'heightened', 'hope', 'horizon', 'hostile', 'human', 'im', 'impressionability', 'inclined', 'infinite', 'interest', 'intimate', 'intricate', 'itself', 'ive', 'judgements', 'last', 'levity', 'life', 'limit', 'little', 'machines', 'made', 'man', 'many', 'marred', 'marshes', 'matter', 'meant', 'men', 'miles', 'mind', 'missing', 'moral', 'more', 'name', 'natures', 'never', 'normal', 'nothing', 'obvious', 'one', 'opened', 'out', 'over', 'parceled', 'people', 'person', 'personality', 'plagiaristic', 'point', 'politician', 'preoccupation', 'preyed', 'privileged', 'privy', 'promises', 'quality', 'quick', 'quivering', 'reaction', 'readiness', 'realized', 'register', 'related', 'remember', 'repeat', 'represented', 'reserve', 'reserved', 'reserving', 'responsiveness', 'revelation', 'revelations', 'right', 'riotous', 'rock', 'romantic', 'scorn', 'secret', 'sense', 'sensitivity', 'series', 'shall', 'shortwinded', 'sign', 'sleep', 'snobbishly', 'something', 'sorrows', 'sort', 'still', 'successful', 'such', 'suggested', 'suppressions', 'temperament', 'temporarily', 'ten', 'terms', 'those', 'thousand', 'told', 'tolerance', 'turned', 'turning', 'unaffected', 'unbroken', 'under', 'understood', 'unequally', 'uniform', 'unjustly', 'unknown', 'unmistakable', 'unsought', 'unusually', 'up', 'usually', 'veteran', 'victim', 'vulnerable', 'wake', 'wanted', 'way', 'wet', 'weve', 'whenever', 'wild', 'world', 'years', 'young', 'younger', 'youve']\n",
    "assert(create_vocab(doc2) == ans2), \"create_vocab failed on {}\".format(doc2)\n",
    "\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1.c** (2 points). Given a list of documents and a vocabulary, let's create bag-of-words vectors for each document.\n",
    "\n",
    "Complete the function `bagofwords(doclist, vocab)`, below. It takes as input a list of documents (`doclist`) and a list of vocabulary words (`vocab`). It will return a list of bag-of-words vectors, with one vector for each document in the input.\n",
    "\n",
    "For instance:\n",
    "\n",
    "```python\n",
    "doc1 = \"create ten different different sample\"\n",
    "doc2 = \"create ten another another example example example\"\n",
    "doc_list = [doc1, doc2]\n",
    "vocab = ['another', 'create', 'different', 'example', 'sample', 'ten']\n",
    "bagofwords(doc_list, vocab) == [[0, 1, 2, 0, 1, 1],\n",
    "                                [2, 1, 0, 3, 0, 1]]\n",
    "```\n",
    "\n",
    "> **Note 0**: Every word in the document must be present in the vocabulary. Therefore you should use the same preprocessing function (`extract_words()`) that was used to create the vocabulary.\n",
    ">\n",
    "> **Note 1**: `bagofwords()` should return a list of vectors, where each vector is a list of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bagofwords(doclist, vocab):\n",
    "    assert (isinstance(doclist, list)), \"bagofwords expects a list of strings as input for doclist.\"\n",
    "    assert (isinstance(vocab, list)), \"bagofwords expects a list of strings as input for vocab.\"\n",
    "    \n",
    "    # i = indexes\n",
    "    # w = words\n",
    "    # enumerate() assigns a number to each element to the list, which we are doing to our vocab list\n",
    "    # This creates a dictionary of index, word pairs for each element in vocab\n",
    "    word_map = {w:i for i, w in enumerate(vocab)}\n",
    "    # Creates nested list\n",
    "    # Inner list for every document, inner list is 0,\n",
    "    # then multiply by the length so that there is one entry for every element\n",
    "    # of the vocabulary\n",
    "    bow = [[0]*len(vocab) for _ in range(len(doclist))] #empty bag of words to be iterated over\n",
    "    # I = row that we're on (think of the inner list as row)\n",
    "    # doc = (think of the outer list as the columns)\n",
    "    for i, doc in enumerate(doclist):\n",
    "        # Words extracted from the document\n",
    "        for word in extract_words(doc): #previous function\n",
    "            # Inner index is for document i that we're on\n",
    "            # Outer index is for the word in that document we're on\n",
    "            # Add one to each word in the document\n",
    "            bow[i][word_map[word]] += 1\n",
    "    return bow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_bagofwords_1",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test Cell: `test_bagofwords_1` (1 point)\n",
    "\n",
    "doc1 = doc_list\n",
    "vocab1 = create_vocab(doc1)\n",
    "vec1 = [0, 1, 2, 0, 1, 1]\n",
    "assert(isinstance(bagofwords(doc1, vocab1),list)), \"Incorrect type of output. bagofwords should return a list of integers.\"\n",
    "assert(bagofwords(doc1, vocab1)[0] == vec1), \"bagofwords failed on {}\".format(doc1)\n",
    "\n",
    "doc2 = [docs[books.index('1984')][-200:]]\n",
    "vocab2 = create_vocab(doc2)\n",
    "vec2 = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "assert(bagofwords(doc2, vocab2)[0] == vec2), \"bagofwords failed on {}\".format(doc2)\n",
    "\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_bagofwords_2",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This test cell will be replaced with one hidden test case.\n",
      "You will only know the result after submitting to the autograder.\n",
      "If the autograder times out, then either your solution is highly\n",
      "inefficient or contains a bug (e.g., an infinite loop).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test Cell: `test_bagofwords_2` (1 point)\n",
    "\n",
    "print(\"\"\"\n",
    "This test cell will be replaced with one hidden test case.\n",
    "You will only know the result after submitting to the autograder.\n",
    "If the autograder times out, then either your solution is highly\n",
    "inefficient or contains a bug (e.g., an infinite loop).\n",
    "\"\"\")\n",
    "\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Let us take a look at the number of words found in our BoW vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prideandprejudice\t: 335 words\n",
      "gatsby           \t: 212 words\n",
      "1984             \t: 401 words\n",
      "littlewomen      \t: 212 words\n",
      "olivertwist      \t: 415 words\n",
      "janeeyre         \t: 126 words\n",
      "hamlet           \t: 519 words\n",
      "kiterunner       \t: 142 words\n",
      "littleprince     \t: 270 words\n",
      "prisonerofazkaban\t: 537 words\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(books)):\n",
    "    bow = bagofwords(docs, create_vocab(docs))\n",
    "    print('{:17s}\\t: {} words'.format(books[i],len(bow[i])-bow[i].count(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "## Normalization (Again?)\n",
    "One of the artifacts you might have noticed from the BoW vectors is that they have very different number of words. This is because the excerpts are of different lengths which may artificially skew the norms of these vectors.  \n",
    "\n",
    "One way to remove this bias is to keep the direction of the vector but normalize the lengths to be equal to one. If the vector is $\\mathbf{v} = \\begin{bmatrix} v_0\\\\ v_1\\\\ \\vdots\\\\ v_{n-1}\\end{bmatrix} \\in \\mathbf{R}^n$, then its unit-normalized version is $\\mathbf{\\hat{v}}$, given by\n",
    "  \n",
    "$$\n",
    "\\mathbf{\\hat{v}} = \\frac{\\mathbf{v}}{\\lVert \\mathbf{v}\\rVert_2} = \\frac{\\mathbf{v}}{\\sqrt{v_0^2 + v_1^2 + \\ldots + v_{n-1}^2}}.\n",
    "$$\n",
    "\n",
    "For instance, recall the BoW vectors from our earlier example:\n",
    "```python\n",
    "bow = [[0, 1, 2, 0, 1, 1],\n",
    "       [2, 1, 0, 3, 0, 1]]\n",
    "```\n",
    "\n",
    "The normalized versions would be\n",
    "```python\n",
    "bow_normalize = [[0.0, 0.3779644730092272, 0.7559289460184544, 0.0, 0.3779644730092272, 0.3779644730092272],\n",
    "                 [0.5163977794943222, 0.2581988897471611, 0.0, 0.7745966692414834, 0.0, 0.2581988897471611]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 1.d** (2 points). Complete the function `bow_normalize(bow)`, below. It should take as input a list of BoW vectors. It should return their unit-normalized versions, per the formula above, also as a **list of vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bow_normalize(bow):\n",
    "    assert(isinstance(bow,list)),\"bow_normalize expects a list of ints as input\"\n",
    "    \n",
    "    # Looking to divide the value by the norm\n",
    "    def norm(x):\n",
    "        # Squared value for each value in the object divided by 1/2\n",
    "        return sum([xi**2 for xi in x]) ** (1/2)\n",
    "    # Apply the above function to every word in the bag of words\n",
    "    bag_norm = [norm(b) for b in bow]\n",
    "    # Nested list again. I = index, bag = element to access the norm\n",
    "    # The inner list divides the value by the normalized value for each element\n",
    "    return [[w/bag_norm[i] for w in bag] for i, bag in enumerate(bow)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_normalize",
     "locked": true,
     "points": "2",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> So far, so good.\n",
      "\n",
      "Running battery of random tests...\n",
      "Input [2, 12, 0, 4, 9, 13, 6]\n",
      "Input [8, 1, 2, 5, 4, 3, 7]\n",
      "Input [6, 12, 13, 4, 2, 11, 10]\n",
      "Input [3, 10, 5, 7, 6, 12, 0]\n",
      "Input [6, 0, 7, 2, 1]\n",
      "Input [1, 9, 8, 4, 0]\n",
      "Input [2, 6, 4, 1, 5]\n",
      "Input [5, 9, 8, 7, 1]\n",
      "Input [1, 6, 7, 4, 8]\n",
      "Input [7, 3, 4, 1, 5]\n",
      "Input [6, 5, 0, 4]\n",
      "Input [1, 5, 4, 7]\n",
      "Input [1, 6, 0, 2]\n",
      "Input [3, 7, 2, 1]\n",
      "Input [0, 2, 7, 4]\n",
      "Input [1, 11, 0, 8, 4, 5]\n",
      "Input [1, 7, 4, 10, 3, 8]\n",
      "Input [4, 0, 5, 10, 7, 9]\n",
      "Input [0, 6, 3, 7, 10, 1]\n",
      "Input [1, 7, 11, 10, 2, 8]\n",
      "Input [3, 1, 6, 5, 13, 7, 9]\n",
      "Input [12, 9, 4, 1, 5, 2, 3]\n",
      "Input [5, 6, 4, 12, 9, 8, 1]\n",
      "Input [8, 0, 2, 11, 10, 1, 4]\n",
      "Input [9, 11, 7, 10, 4, 0, 1]\n",
      "Input [1, 2, 4, 0, 11, 12, 5]\n",
      "Input [1, 2, 4]\n",
      "Input [3, 0, 4]\n",
      "Input [4, 0, 5]\n",
      "Input [3, 4, 5]\n",
      "Input [0, 5, 4, 1, 9]\n",
      "Input [1, 8, 9, 0, 6]\n",
      "Input [8, 4, 3, 7, 5]\n",
      "Input [3, 8, 0, 1, 4]\n",
      "Input [1, 3, 7, 0]\n",
      "Input [5, 1, 7, 2]\n",
      "Input [6, 2, 1, 7]\n",
      "Input [0, 6, 4, 5]\n",
      "Input [3, 1, 0, 5]\n",
      "Input [4, 5, 2, 0]\n",
      "Input [4, 5, 0, 3]\n",
      "Input [0, 5, 7, 4]\n",
      "Input [0, 4, 6, 3]\n",
      "Input [5, 1, 7, 2]\n",
      "Input [3, 7, 4, 6]\n",
      "Input [2, 1, 7, 4]\n",
      "Input [9, 1, 5, 7, 0]\n",
      "Input [3, 0, 4, 7, 9]\n",
      "Input [5, 1, 0, 7, 9]\n",
      "Input [8, 5, 2, 1, 7]\n",
      "Input [9, 1, 2, 8, 6]\n",
      "Input [1, 9, 8, 4, 6]\n",
      "Input [9, 3, 7, 8, 0]\n",
      "Input [6, 3, 9, 7, 8]\n",
      "Input [0, 5, 2, 6, 1]\n",
      "Input [7, 5, 4, 0, 9]\n",
      "Input [0, 2]\n",
      "Input [2, 0]\n",
      "Input [0, 2]\n",
      "Input [0, 2, 3]\n",
      "Input [3, 4, 0]\n",
      "Input [4, 3, 1]\n",
      "Input [2, 0, 3]\n",
      "Input [5, 2, 0]\n",
      "Input [4, 3, 6, 1]\n",
      "Input [7, 2, 6, 0]\n",
      "Input [5, 4, 3, 2]\n",
      "Input [9, 1, 0, 8, 2]\n",
      "Input [6, 8, 9, 1, 2]\n",
      "Input [7, 0, 6, 2, 5]\n",
      "Input [7, 6, 1, 8, 4]\n",
      "Input [5, 1, 4]\n",
      "Input [1, 4, 3]\n",
      "Input [0, 3, 2]\n",
      "Input [1, 4, 5]\n",
      "Input [1, 5, 0]\n",
      "Input [2, 4, 3]\n",
      "Input [7, 8, 10, 4, 5, 3]\n",
      "Input [8, 2, 10, 6, 1, 11]\n",
      "Input [9, 6, 8, 7, 3, 5]\n",
      "Input [11, 2, 7, 9, 6, 4]\n",
      "Input [4, 2, 0]\n",
      "Input [3, 4, 5]\n",
      "Input [2, 5, 1]\n",
      "Input [3, 5, 1]\n",
      "Input [2, 0, 1]\n",
      "Input [1, 0, 4]\n",
      "Input [10, 8, 5, 4, 1, 11]\n",
      "Input [10, 8, 6, 1, 9, 5]\n",
      "Input [8, 9, 7, 4, 0, 1]\n",
      "Input [8, 2, 0, 9, 10, 1]\n",
      "Input [9, 10, 5, 0, 7, 2]\n",
      "Input [9, 11, 3, 4, 6, 7]\n",
      "Input [3, 0]\n",
      "Input [1, 0]\n",
      "Input [0, 2]\n",
      "Input [2, 1]\n",
      "Input [0, 1]\n",
      "Input [3, 1]\n",
      "\n",
      " (Passed!)\n"
     ]
    }
   ],
   "source": [
    "bow0 = [[1, 2, 3, 1, 1], [2, 2, 2, 2, 0]]\n",
    "nbow0 = [[0.25, 0.5, 0.75, 0.25, 0.25], [0.5, 0.5, 0.5, 0.5, 0]]\n",
    "\n",
    "ans0 = bow_normalize(bow0)\n",
    "assert(isinstance(ans0,list)), \"Incorrect type of output. bow_normalize should return a list of floats.\"\n",
    "\n",
    "assert(nbow0[0] == ans0[0]), \"bow_normalize failed on {}\".format(bow0[0])\n",
    "assert(nbow0[1] == ans0[1]), \"bow_normalize failed on {}\".format(bow0[1])\n",
    "\n",
    "bow1 = [[0, 1, 2, 0, 1, 1],\n",
    "       [2, 1, 0, 3, 0, 1]]\n",
    "nbow1 = [[0.0, 0.3779644730092272, 0.7559289460184544, 0.0, 0.3779644730092272, 0.3779644730092272],\n",
    "    [0.5163977794943222, 0.2581988897471611, 0.0, 0.7745966692414834, 0.0, 0.2581988897471611]]\n",
    "\n",
    "ans1 = bow_normalize(bow1)\n",
    "assert(nbow1[0] == ans1[0]), \"bow_normalize failed on {}\".format(bow1[0])\n",
    "assert(nbow1[1] == ans1[1]), \"bow_normalize failed on {}\".format(bow1[1])\n",
    "\n",
    "print(\"==> So far, so good.\")\n",
    "# Some Random Instances\n",
    "def check_bow_normalize_random():\n",
    "    from random import choice, sample\n",
    "    \n",
    "    vec_len =  choice(range(2,8))\n",
    "    nvecs = choice(range(3,7))\n",
    "    rvecs = []\n",
    "    for _ in range(nvecs):\n",
    "        rvecs.append(sample(range(2*vec_len),vec_len))\n",
    "        \n",
    "    unit_rvecs = bow_normalize(rvecs)\n",
    "    \n",
    "    for i in range(len(unit_rvecs)):\n",
    "        print(\"Input {}\".format(rvecs[i]))\n",
    "        u = unit_rvecs[i]\n",
    "        ans = 1.0;\n",
    "        \n",
    "        for ui in u:\n",
    "            ans = ans - (ui*ui)\n",
    "            \n",
    "        assert (math.isclose(ans,0,rel_tol=1e-6,abs_tol=1e-8)), \"ERROR: Your output is incorrect {}\".format(u)\n",
    "        \n",
    "print(\"\\nRunning battery of random tests...\")\n",
    "for _ in range(20):\n",
    "    check_bow_normalize_random()\n",
    "\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "(_Aside_) **Sparsity of BoW vectors.** As an aside, run the next cell to see the BoW vectors are actually quite _sparse_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of entries which are zero: 85.9 %\n"
     ]
    }
   ],
   "source": [
    "literature_vocab = create_vocab(docs)\n",
    "bow = bagofwords(docs, literature_vocab)\n",
    "nbow = bow_normalize(bow)\n",
    "numterms = len(docs)*len(literature_vocab)\n",
    "numzeros = sum([b.count(0) for b in nbow])\n",
    "print(\"Percentage of entries which are zero: {:.1f} %\".format(100*numzeros/numterms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "> If everything is correct, you'll see that the BoW vectors are sparse, with about 85-86% of the components being zeroes. Therefore, we could in principle save a lot of space by only storing the non-zeroes. While we do not exploit this fact in our current example it is useful to think about these costs while running analytics at scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "# Part 2. Comparing Documents\n",
    "\n",
    "Now we have normalized vector versions of each document, we can use a standard similarity measure to compare vectors. For this question, we shall use the *inner product*. Recall that the inner product of two vectors $a,b \\in \\mathbf{R}^n$ is defined as,\n",
    "\n",
    "$$<a,b> = \\Sigma_{i=0}^{n-1} a_i b_i$$\n",
    "\n",
    "For example,\n",
    "\n",
    "$$<[1,-1,3], [2,4,-1]> = (1\\times2)+(-1\\times4)+(3\\times-1)=-5$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Exercise 2.a** (1 point) Complete the function `inner_product(a, b)` which takes two vectors, `a` and `b`, both represented as lists, and returns their inner product.\n",
    "\n",
    "> Note: `inner_product(a, b)` should return a value of type `float`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inner_product(a,b):\n",
    "    assert (isinstance(a, list)), \"inner_product expects a list of floats/ints as input for a.\"\n",
    "    assert (isinstance(b, list)), \"inner_product expects a list of floats/ints as input for b.\"\n",
    "    assert len(a) == len(b), \"inner_product should be called on vectors of the same length.\"\n",
    "    \n",
    "    # Pair of corresponding elements of each vector, take the product\n",
    "    # Result has to be a float\n",
    "    return sum([float(ai * bi) for ai, bi in zip(a, b)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_inner_product",
     "locked": true,
     "points": "1",
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " (Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test Cell: `test_inner_product` (0.5 point)\n",
    "\n",
    "vec1a = [1,-1,3]\n",
    "vec1b = [2,4,-1]\n",
    "ans1 = -5\n",
    "assert (isinstance(inner_product(vec1a,vec1b),float)), \"Incorrect type of output. inner_product should return a float.\"\n",
    "assert (inner_product(vec1a,vec1b) == ans1), \"inner_product failed on inputs {} and {}\".format(vec1a,vec1b)\n",
    "assert (inner_product(vec1b,vec1a) == ans1), \"inner_product failed on inputs {} and {}\".format(vec1b,vec1a)\n",
    "\n",
    "vec2a = [0,2,1,9,-1]\n",
    "vec2b = [17,4,1,-1,0]\n",
    "ans2 = 0\n",
    "assert (inner_product(vec2a,vec2b) == ans2), \"inner_product failed on inputs {} and {}\".format(vec2a,vec2b)\n",
    "assert (inner_product(vec2b,vec2a) == ans2), \"inner_product failed on inputs {} and {}\".format(vec2b,vec2a)\n",
    "\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "We can use the `inner_product()` as a measure of similarity between documents! (_Recall the linear algebra refresher in Topic 3_.) In particular, since our normalized BoW vectors are \"direction\" vectors, the inner product measures how closely two vectors point in the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2.b** (1 point). Now we can finally answer our initial question: which book excerpts are similar to each other? Complete the function `most_similar(nbows, target)`, below, to answer this question. In particular, it should take as input the normalized BoW vectors created in the previous part, as well as a target excerpt index $i$. It should return most index of the excerpt most similar to $i$.\n",
    "\n",
    "> **Note 0.** Ties in scores are won by the smaller index. For example, if excerpt 2 and excerpt 7 both equally similar to the target excerpt 8, then return 2 as the most similar excerpt.\n",
    ">\n",
    "> **Note 1.** Your `most_similar()` function should return a value of type `int`.\n",
    "\n",
    "> **Note 2.** The test cell refers to hidden tests, but in fact, the test is not hidden per se. Instead, we are hashing the strings returned by your solution to be able to check your answer without revealing it to you directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def most_similar(nbows, target):\n",
    "    assert (isinstance(nbows,list)), \"most_similar expects list as input for nbows.\"\n",
    "    assert (isinstance(target,int)), \"most_similar expects integer as input for target.\"\n",
    "    \n",
    "    sim_tuples = [(i, inner_product(bow, nbows[target])) \\\n",
    "                  # For every index, bag in enumerated nbows element\n",
    "                  for i, bow in enumerate(nbows)  \n",
    "                  if i != target]\n",
    "    return max(sim_tuples, key = lambda x: x[1])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": true,
     "grade_id": "test_most_similar",
     "locked": true,
     "points": "1",
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For book '1984', you calculated 'kiterunner' as most similar.\n",
      "For book 'gatsby', you calculated 'prideandprejudice' as most similar.\n",
      "For book 'hamlet', you calculated 'prideandprejudice' as most similar.\n",
      "For book 'janeeyre', you calculated 'prideandprejudice' as most similar.\n",
      "For book 'kiterunner', you calculated '1984' as most similar.\n",
      "For book 'littleprince', you calculated 'littlewomen' as most similar.\n",
      "For book 'littlewomen', you calculated 'littleprince' as most similar.\n",
      "For book 'olivertwist', you calculated '1984' as most similar.\n",
      "For book 'prideandprejudice', you calculated 'hamlet' as most similar.\n",
      "For book 'prisonerofazkaban', you calculated '1984' as most similar.\n",
      "\n",
      " (Passed!)\n"
     ]
    }
   ],
   "source": [
    "# Test Cell: `test_most_similar` (1 point)\n",
    "literature_vocab = create_vocab(docs)\n",
    "bow = bagofwords(docs, literature_vocab)\n",
    "nbow = bow_normalize(bow)\n",
    "\n",
    "# Start with two basic cases:\n",
    "target1 = books.index('1984') \n",
    "ans1 = books.index('kiterunner')\n",
    "assert (isinstance(most_similar(nbow,target1),int)), \"most_similar should return integer.\"\n",
    "assert (most_similar(nbow,target1) == ans1), \"most_similar failed on input {}\".format(books[target1])\n",
    "\n",
    "target2 = books.index('prideandprejudice')\n",
    "ans2 = books.index('hamlet')\n",
    "assert (most_similar(nbow,target2) == ans2), \"most_similar failed on input {}\".format(books[target2])\n",
    "\n",
    "# Check the rest via obscured, hashed solutions\n",
    "###\n",
    "### AUTOGRADER TEST - DO NOT REMOVE\n",
    "###\n",
    "def check_most_similar_solns():\n",
    "    from problem_utils import make_hash, open_file\n",
    "    literature_vocab = create_vocab(docs)\n",
    "    bow = bagofwords(docs, literature_vocab)\n",
    "    nbow = bow_normalize(bow)\n",
    "    with open_file(\"most_similar_solns.csv\", \"rt\") as fp_soln:\n",
    "        for line in fp_soln.readlines():\n",
    "            target_name, soln_hashed = line.strip().split(',')\n",
    "            target_id = books.index(target_name)\n",
    "            your_most_sim_id = most_similar(nbow, target_id)\n",
    "            assert isinstance(your_most_sim_id, int), f\"Your function returns a value of type {type(your_most_sim_id)}, not an integer\"\n",
    "            assert 0 <= your_most_sim_id < len(nbow), f\"You returned {your_most_sim_id}, which is an invalid value (it should be between 0 and {nbow})\"\n",
    "            your_most_sim_name = books[your_most_sim_id]\n",
    "            print(f\"For book '{target_name}', you calculated '{your_most_sim_name}' as most similar.\")\n",
    "            your_most_sim_name_hashed = make_hash(your_most_sim_name)\n",
    "            assert your_most_sim_name_hashed == soln_hashed, \"==> ERROR: Unfortunately, your returned value does not appear to match our reference solution.\"\n",
    "\n",
    "check_most_similar_solns()\n",
    "print(\"\\n (Passed!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "Now let's have a look at the documents most similar to each other, according to your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prideandprejudice is most similar to hamlet !\n",
      "gatsby is most similar to prideandprejudice !\n",
      "1984 is most similar to kiterunner !\n",
      "littlewomen is most similar to littleprince !\n",
      "olivertwist is most similar to 1984 !\n",
      "janeeyre is most similar to prideandprejudice !\n",
      "hamlet is most similar to prideandprejudice !\n",
      "kiterunner is most similar to 1984 !\n",
      "littleprince is most similar to littlewomen !\n",
      "prisonerofazkaban is most similar to 1984 !\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(books)):\n",
    "    jdx = most_similar(nbow,idx)\n",
    "    print(books[idx],\"is most similar to\",books[jdx],\"!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "grade": false,
     "locked": true,
     "solution": false
    }
   },
   "source": [
    "**Fin!** You’ve reached the end of this part. Don’t forget to restart and run all cells again to make sure it’s all working when run in sequence; and make sure your work passes the submission process. Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "nteract": {
   "version": "0.15.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
